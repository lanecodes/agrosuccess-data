{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Land Cover Type (LCT) time series from EPD data\n",
    "\n",
    "The purpose of this notebook is to :\n",
    "1. Load pollen abundance time-series data extracted from the European Pollen Database for a selection of sites I am studying in the development of my PhD thesis.\n",
    "2. Explore, consider the limitations of, and clean that data.\n",
    "3. Support the systematic assignment of pollen types identified in the empirical data to the categorical land-cover types which will be represented in my simulation models. This is a form of modelling in itself, and serves as an abstraction couched in terms of the notion of a plant functional type. That is, plant _species_ which are postulated to be functionally identical as far as the model is concerned are assigned to the same plant functional group. This will be achieved using regular expressions to embelish the data in a pandas dataframe.\n",
    "4. Produce, for each of my empirical study sites, time-series of the proportion of landscape occupied for each of the functional groups represented in the model for the duration of time for which there is abundance data for each study site. This will be presented in the form of a `.csv` file and a plot for each study site. These files will also include first and second derivatives of pollen abundance percentage at each time step.\n",
    "\n",
    "The only input required to run this notebook is a path to the file `site_pollen_abundance_ts.csv` which is output from [`epd-query`](https://github.com/lanecodes/epd-query)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "from typing import Dict, List\n",
    "\n",
    "import unidecode\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from aslib import AgroSuccessLct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd = os.getcwd().split('/')[-1]\n",
    "in_pollen_abundance = pwd == 'pollen-abundance'\n",
    "TMP_DIR = Path('../tmp') if in_pollen_abundance else Path('tmp')\n",
    "OUTPUT_DIR = Path('../outputs') if in_pollen_abundance else Path('outputs')\n",
    "PLOTS_DIR = OUTPUT_DIR / 'plots'\n",
    "PLOTS_DIR.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load pollen data from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epd_data = pd.read_csv(TMP_DIR / 'site_pollen_abundance_ts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epd_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epd_data.groupby(['sitename', 'e_']).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Explore, condider the limitations of, and clean pollen core data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check numbers of samples in each core, narrow core selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data for one of the three Navarres cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nav1 = epd_data[epd_data['e_'] == 469]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarise_core(df, name):\n",
    "    print(f'{df[\"sample_\"].unique().size} samples in core {name}')\n",
    "    print(f'{len(df.index)} records across all samples in core {name}')\n",
    "    print(f'Top 10 varcodes in core {name}:')\n",
    "    print(df.groupby('varcode')['varcode'].count().nlargest(10))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarise_core(nav1, 'nav1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that one of the top pollen codes in the database for this sediment core corresponds to [pollen spike](https://quantpalaeo.wordpress.com/2017/07/28/pollen-spikes/) or is unspecified with `varcode` values of `conc.spk` and `...`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More troublingly, navares core 469, NAVA1 has only 15 samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nav2 = epd_data[epd_data['e_'] == 470]\n",
    "summarise_core(nav2, 'nav2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NAVA2 has only 30 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nav3 =  epd_data[epd_data['e_'] == 471]\n",
    "summarise_core(nav3, 'nav3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NAVA3 has 191 samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going forward, I'll prefer NAVA3 over NAVA1 and NAVA2 since it contains more samples. If I find something which makes NAVA3 seem unreliable, I may reconsider. For now, drop NAVA1 and NAVA2 from the `epd_data` dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epd_data = epd_data[~epd_data['e_'].isin([469, 470])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at top ten pollen contributing species for each study site, remove sediment spike"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top_species(epd_data):\n",
    "    for ssite in epd_data['sitename'].unique():\n",
    "        print('\\n'+ssite)\n",
    "        df = epd_data[epd_data['sitename']==ssite]\n",
    "        df = df.groupby(['var_', 'varcode', 'varname']).agg({'count' : 'sum'})\n",
    "        print(df.sort_values(by='count', ascending=False).head(5))\n",
    "    del df\n",
    "\n",
    "print_top_species(epd_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Navarres alone seems to have a lot of pollen spike in it. Also Monte Areo mire and Charco da Candieira have Lycopodium spike added. See [here](https://palynology.wordpress.com/2012/10/07/pollen-spike/) for background on pollen spike. To keep analyses between sites consistent, I will exclude these. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_varcodes(df: pd.DataFrame, varcodes: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"Remove rows corresponding to specified varcodes from epd DF.\"\"\"\n",
    "    return df[~df['varcode'].isin(varcodes)]\n",
    "\n",
    "\n",
    "def remove_varcodes_test_df():\n",
    "    return pd.DataFrame({\n",
    "        'varcode': ['goodvar1', 'badvar1', 'badvar2', 'goodvar2'],\n",
    "        'count': np.random.randint(0, 4000, size=4)\n",
    "    })\n",
    "\n",
    "\n",
    "def test_remove_varcodes(test_df):\n",
    "    res_df = remove_varcodes(test_df, ['badvar1', 'badvar2'])\n",
    "    assert res_df.iloc[0]['varcode'] == 'goodvar1'\n",
    "    assert res_df.iloc[1]['varcode'] == 'goodvar2'\n",
    "    assert len(res_df.index) == 2\n",
    "    \n",
    "test_remove_varcodes(remove_varcodes_test_df())   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude_pollen_spike = True\n",
    "if exclude_pollen_spike:\n",
    "    epd_data = remove_varcodes(\n",
    "        epd_data, ['Spi/tab', 'Lyc(ad)', 'Lyc(ct)', 'Lyc']\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also note that San Rafael has a significant proportion of Botryococcus in its samples. This is a type of green algae. Since this doesn't correspond to any _land_ plant species, we exclude it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aquatic_plant_codes = [\n",
    "    'Bry',\n",
    "    'Zyg-T',\n",
    "    'Spr-T',\n",
    "    'Pot',      # Potamogeton, aquatic plant\n",
    "    'Clo.i-T',  # Closterium idiosporum, green algae\n",
    "    'Spi.cf.s', # Spirogyra cf. scrobiculata, green algae\n",
    "    'Trl.s',    # Trilete spore(s),  not from modern terrestrial plant\n",
    "]\n",
    "\n",
    "exclude_non_land_plants = True\n",
    "if exclude_non_land_plants:\n",
    "    epd_data = remove_varcodes(epd_data, aquatic_plant_codes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identified lots of moss (Sphagnum) in, e.g. Atxuri. Exclude this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude_mosses = True\n",
    "if exclude_mosses:\n",
    "    epd_data = remove_varcodes(epd_data, ['Sph'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fungal spores such as Glomus turn up in Navarres. Exclude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fungal_species_codes = [\n",
    "    'Glomus',\n",
    "    'Pos',  # Polyadosporites, fungal spore http://www.redalyc.org/html/454/45437346003/index.html\n",
    "]\n",
    "\n",
    "exclude_fungi = True\n",
    "if exclude_fungi:\n",
    "    epd_data = remove_varcodes(epd_data, fungal_species_codes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove records corresponding to pollen which could not be identified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unrecognised_species_codes = [\n",
    "    'Ind.unkn',  # found in navarres\n",
    "    'T16C',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude_unrecognised = True\n",
    "if exclude_unrecognised:\n",
    "    epd_data = remove_varcodes(epd_data, unrecognised_species_codes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, `epd_data` contains entries for all:\n",
    "1. sediment cores\n",
    "2. samples (depths/ ages)\n",
    "3. species (careful to exclude pollen spike)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epd_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Give each site an easily typed `sitecode` to refer to as an index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It will be convenient to be able to refer to sites as an index. To make these easy to type, create a `sitecode` column which strips out spaces and removes any unicode names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(epd_data['sitename'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epd_data['sitecode'] = (\n",
    "    epd_data['sitename']\n",
    "    .apply(unidecode.unidecode)\n",
    "    .str.replace(' ', '_')\n",
    "    .str.lower()\n",
    ")\n",
    "\n",
    "print(epd_data.sitecode.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epd_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop unnecessary columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I plan to use `sitename` as an index going forward because it's natural to think in terms of study sites. This means I don't need other information in the dataframe I take forward in my analyses at the study site level of detail. So this information can easily be rerieved if needs be when debugging, I save this to disk and remove the extra columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_meta_fields = ['sitecode', 'sitename', 'site_', 'sigle', 'e_', 'chron_']\n",
    "site_meta = epd_data.groupby(site_meta_fields).size().rename('num_records')\n",
    "site_meta.to_csv(OUTPUT_DIR / 'site_metadata.csv', encoding='utf8', header=True)\n",
    "epd_data = epd_data.drop(\n",
    "    [x for x in site_meta_fields if x != 'sitecode'], axis=1\n",
    ")\n",
    "epd_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`sample_` (a database key from the EPD) is also redundant at this point, since we can idenify each sample from its `agebp`. Similarly each variable (pollen species) is uniquely identified by its `varcode` so we can also drop `var_`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epd_data = epd_data.drop(['sample_', 'var_'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epd_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check `agebp` and `count` can be converted to `int` without loss of data, and do the conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_field_to_int(df: pd.DataFrame, field: str) -> pd.DataFrame:\n",
    "    \"\"\"Convert named float field to int if no data would be lost.\"\"\"\n",
    "    assert (~df[field].isna()).all(), f'missing data found in {field}'\n",
    "    assert ((df[field] - df[field].astype(int)) == 0).all(), (\n",
    "     f'casting {field} to int caused loss of data'\n",
    "    )\n",
    "    df[field] = df[field].astype(int)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epd_data = (\n",
    "    epd_data\n",
    "    .pipe(lambda df: convert_field_to_int(df, 'agebp'))\n",
    "    .pipe(lambda df: convert_field_to_int(df, 'count'))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epd_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a unique index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epd_data = epd_data.set_index(['sitecode', 'agebp', 'varcode']).sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find index is unexpectedly not unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epd_data.index.is_unique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check which sites duplicates are coming from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epd_data[epd_data.index.duplicated()].groupby(level=['sitecode']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pct_affected = 107 / len(epd_data.loc['charco_da_candieira'].index) * 100\n",
    "print(f'{round(pct_affected, 2):.2f}% of charco_da_candieira entries are duplicates')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As less than 1% of Charco da Candieira sample/ species combinations are affected, we will simply assume that where multiple entries are associated for a species in a single sample, the correct count is obtained by summing any duplicates. No other site's data are affected by this issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_index_len = len(epd_data.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "varcode_to_varname_df = (\n",
    "    epd_data.reset_index(level='varcode')[['varcode', 'varname']]\n",
    "    .drop_duplicates()\n",
    "    .set_index('varcode')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epd_data = (\n",
    "    epd_data['count']\n",
    "    .groupby(level=['sitecode', 'agebp', 'varcode']).sum()\n",
    "    .to_frame()\n",
    "    .join(varcode_to_varname_df)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert initial_index_len - len(epd_data.index) == 107\n",
    "assert epd_data.index.is_unique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rename `count` to avoid an understandable but irritating namespace collision with the `pd.Series.count` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epd_data = epd_data.rename(columns={'count': 'pcount'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epd_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epd_data.loc['navarres'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epd_data.groupby(level=['sitecode', 'agebp']).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`epd_data` is now prepped and ready to use for subsequent analyses. Serialise a csv file so it can be retrieved without rerunning the above cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epd_data.to_csv(OUTPUT_DIR / 'clean_epd_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Relate identified pollen species with model-dependent plant functional types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    epd_data\n",
    "except NameError:\n",
    "    epd_data = (\n",
    "        pd.read_csv(OUTPUT_DIR / 'clean_epd_data.csv')\n",
    "        .set_index(['sitecode', 'agebp', 'varcode'])\n",
    "    )   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve a list of unique `varname`-s found amongst the sediment cores analysed thus far in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_species = (\n",
    "    epd_data.reset_index()[['varname', 'varcode']].drop_duplicates()\n",
    "    .set_index('varcode')\n",
    ")\n",
    "unique_species.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the most common species for each study site\n",
    "\n",
    "The objective is to ensure that approximately 90% of counted pollen is assigned to one of the following land cover type groups:\n",
    "\n",
    "- Shrubland: includes grasses (Poaceae, formerly Gramineae, family), and juniper (genus Juniperus, belongs to cypress family Cupressaceae).\n",
    "- Pine forest: anything belonging to the Pinus genus\n",
    "- Deciduous forest: Beech family, Fagaceae and Chestnut (Castanea genus)\n",
    "- Oak forest: anything belonging to the Quercus genus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find percentage of each study site's total contributed by each species. These are the species whose mapping to land cover types are most important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    epd_data.groupby(['sitecode', 'varcode'])['pcount'].sum().to_frame()\n",
    "    .pipe(lambda df: df.join(df.groupby('sitecode')['pcount']\n",
    "                             .sum().rename('site_total')))\n",
    "    .assign(species_pct=lambda df: df['pcount'] / df['site_total'] * 100)\n",
    "    .drop(columns='site_total')\n",
    "    .groupby('sitecode')['species_pct'].nlargest(10)\n",
    "    .reset_index(0, drop=True)\n",
    "    .to_frame()\n",
    "    .join(unique_species)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify land-cover types with pollen species"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim in this section is to construct a dictionary whose keys are land cover types included in my simulation models, and whose values are regular expressions which match the names of species contributing to those land cover types. This dictionary will then be used to say: if _this_ pattern is found in a species name, map it to _this_ land cover type.\n",
    "\n",
    "The following land cover types are included in simulations, but not in the land cover type categories used in this notebook:\n",
    "\n",
    "1. Water/Quarry\n",
    "2. Burnt\n",
    "3. Depleated agricultural land\n",
    "3. Barley\n",
    "4. Wheat\n",
    "5. Transition forest\n",
    "\n",
    "Land cover types 1-3 above don't produce any pollen. Barley and wheat produce grass pollen. This belongs to the Poaceae (formerly known as Gramineae) family, and is assumed to contribute to 'Shrubland'. There is no depleated agricultural land, barley or wheat land cover present at the beginning of a simulation, as these are anthropogenically induced land cover types.\n",
    "\n",
    "I don't map pollen to the 'Transition forest' land cover type because this type is a mixture of pine and oak forest. When comparing simulation outputs to empirical pollen abundance, I will assume transition forest simulation cells contribute half a cell of pine forest pollen and half a cell of oak forest pollen. When generating Neutral Landscape models from pollen abundance, I will assume that no cells start off as transition forest, and allow transition forest cells to be introduced by a model 'burn in' period. An alternative proposal might be to change my modelling approach such that I effectively integrate out the transition forest state, so we create the possibility of transitioning directly between pine and oak, subject to the kind of environmental conditions which would support transition forest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Map land land cover types to species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from taxa import POLLEN_LCT_MAPS, compose_regexs, SpeciesGroup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write regex to species mapping to csv and latex files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sg_list_to_df(sgs: List[SpeciesGroup]):\n",
    "    \"\"\"Convert list of SpeciesGroup objects to a dataframe.\"\"\"\n",
    "    df = pd.DataFrame([x.__dict__ for x in sgs])\n",
    "    return df.fillna(np.nan)   \n",
    "\n",
    "species_regex_df = (\n",
    "    pd.concat({k: sg_list_to_df(v) for k, v in POLLEN_LCT_MAPS.items()})\n",
    "    .reset_index(1, drop=True)\n",
    "    .pipe(lambda df: df.set_index(pd.Index(df.index, name='Functional type')))\n",
    "    .rename(columns={'regex': 'Regular expression', 'desc': 'Description',\n",
    "                     'note': 'Note'})\n",
    "    .set_index('Description', append=True)\n",
    "    .sort_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "species_regex_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "species_regex_df.to_csv(OUTPUT_DIR / 'species_regex.csv')\n",
    "species_regex_df.drop(columns='Note').to_latex(\n",
    "    OUTPUT_DIR / 'species_regex.tex',\n",
    "    longtable=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function which, given a species name, returns a list of land cover types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lct(species_name: str, pol_lct_dict: Dict[str, str],\n",
    "            verbose=False) -> str:\n",
    "    \"\"\"Given a species name, map it to a land cover type.\n",
    "    \n",
    "    Throw a ValueError if species name matches more than one land cover type.\n",
    "    \"\"\"\n",
    "    lcts = []\n",
    "    for lct_name, regex in pol_lct_dict.items():\n",
    "        if re.match(regex, species_name, re.IGNORECASE):\n",
    "            lcts.append(lct_name)\n",
    "            if verbose:\n",
    "                print(regex + ' matches ' + species_name)\n",
    "    \n",
    "    if len(lcts) > 1:\n",
    "        raise ValueError('Species name {0} matched multiple land cover type '\n",
    "                         'regex strings: {1}'.format(species_name, lcts))\n",
    "    if len(lcts) == 0:\n",
    "        return None\n",
    "\n",
    "    return lcts[0]\n",
    "\n",
    "\n",
    "def test_get_lct():\n",
    "    regex_to_lct_map = {lct_name: compose_regexs(\n",
    "        [x.regex for x in species_group_list]\n",
    "    ) for lct_name, species_group_list in POLLEN_LCT_MAPS.items()}\n",
    "    \n",
    "    def species_name_test(species_name, expected_lct):\n",
    "        determined_lct = get_lct(species_name, regex_to_lct_map)\n",
    "        assert  determined_lct == expected_lct, (\n",
    "            f\"expected '{expected_lct}' to be lct for species \"\n",
    "            f\"'{species_name}' but got '{determined_lct}' instead.\"\n",
    "        )\n",
    "        \n",
    "    species_name_test('Quercus ilex-type', 'oak_forest')\n",
    "    species_name_test('Quercus', 'oak_forest')\n",
    "    species_name_test('Pinus pinaster-type', 'pine_forest')\n",
    "    species_name_test('Pinus', 'pine_forest')\n",
    "    species_name_test('Rumex crispus-type', 'shrubland')\n",
    "    species_name_test('Compositae subf. Cichorioideae', 'shrubland')\n",
    "    species_name_test('Erica arborea-type', 'shrubland')\n",
    "    species_name_test('Ericaceae', 'shrubland')\n",
    "    species_name_test('Polypodium vulgare-type', 'shrubland')\n",
    "    \n",
    "test_get_lct()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply `get_lct` to each species included in the chronology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex_to_lct_map = {lct_name: compose_regexs([x.regex \n",
    "                                              for x in species_group_list])\n",
    "                    for lct_name, species_group_list in POLLEN_LCT_MAPS.items()}\n",
    "unique_species['lct'] = unique_species.varname.apply(\n",
    "    lambda x: get_lct(x, regex_to_lct_map)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_species = unique_species[unique_species['lct'].notnull()]\n",
    "mapped_species.to_csv(TMP_DIR / 'species_to_landcover_mapping.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each study site, find the percentage of pollen contributed by each species to each sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epd_data = (\n",
    "    epd_data\n",
    "    .join(epd_data\n",
    "          .groupby(level=['sitecode', 'agebp'])['pcount'].sum()\n",
    "          .rename('sample_tot'))\n",
    "    .assign(species_pct=lambda df: df['pcount'] / df['sample_tot'] * 100)\n",
    "    .drop(columns='sample_tot')\n",
    ")\n",
    "\n",
    "assert (epd_data.groupby(level=['sitecode', 'agebp'])['species_pct'].sum()\n",
    "        - 100 < 0.00001).all(), (\n",
    "    'site/ sample percentage totals should equal 100'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epd_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add `lct` to index via `unique_species`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epd_data = (\n",
    "    epd_data\n",
    "    .join(unique_species.drop(columns='varname'))\n",
    "    .assign(lct=lambda df: df['lct'].fillna('not_specified'))\n",
    "    .set_index('lct', append=True)\n",
    "    .swaplevel(3, 2)\n",
    "    .sort_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epd_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate proportion of pollen, for each study site, accounted for by land-cover type mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggregate `epd_data` from species level to land cover type level "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_lct_pct_df = (\n",
    "    epd_data\n",
    "    .groupby(level=['sitecode', 'lct'])['pcount'].sum()\n",
    "    .rename('lct_total_count').to_frame()\n",
    "    .pipe(lambda df: df.join(df.groupby(level='sitecode')['lct_total_count']\n",
    "                             .sum().rename('site_total')))\n",
    "    .assign(site_lct_pct=lambda df: (df['lct_total_count'] \n",
    "                                     / df['site_total'] * 100))\n",
    "    .loc[:, 'site_lct_pct']\n",
    "    .unstack()\n",
    "    .loc[:, ['shrubland', 'pine_forest', 'oak_forest', 'deciduous_forest', 'not_specified']]\n",
    ")\n",
    "\n",
    "assert (total_lct_pct_df.sum(1) - 100 < 0.00001).all(), (\n",
    "    'per-sample total lct contributions should total 100%'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write total percentage of pollen corresponding to each land cover type to outputs to facilitate subsequent plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_lct_pct_df.to_csv(OUTPUT_DIR / 'site_total_lct_pct.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_name_map = {\n",
    "    'algendar': 'Algendar',\n",
    "    'atxuri': 'Atxuri',\n",
    "    'charco_da_candieira': 'Charco da\\nCandieira',\n",
    "    'monte_areo_mire': 'Monte Areo\\nmire',\n",
    "    'navarres': 'NavarrÃ©s',\n",
    "    'san_rafael': 'San Rafael'\n",
    "}\n",
    "\n",
    "lct_name_map = {\n",
    "    'shrubland': 'Shrubland',\n",
    "    'pine_forest': 'Pine',\n",
    "    'oak_forest': 'Oak',\n",
    "    'deciduous_forest': 'Deciduous',\n",
    "    'not_specified': 'Not allocated'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "font = {'weight' : 'normal',\n",
    "        'size'   : 13}\n",
    "matplotlib.rc('font', **font)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lct_color_list = [x.color.hex_code for x \n",
    "                  in [AgroSuccessLct.SHRUBLAND, AgroSuccessLct.PINE,\n",
    "                  AgroSuccessLct.OAK, AgroSuccessLct.DECIDUOUS]] + ['w']\n",
    "f, ax = plt.subplots(figsize=(9, 5))\n",
    "plot_df = total_lct_pct_df.rename(site_name_map, axis=0).rename(lct_name_map, axis=1)\n",
    "plot_df.plot(kind='bar', stacked=True, ax=ax, color=lct_color_list, linewidth=.5, edgecolor='k')\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1.0, 0.45), frameon=False)\n",
    "plt.xticks(rotation=45)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.set_xlabel(None);\n",
    "ax.tick_params(axis=u'x', length=0)\n",
    "ax.set_ylabel('% pollen allocated to PFT');\n",
    "plt.tight_layout()\n",
    "plt.savefig(PLOTS_DIR / 'pct_pollen_allocated_lct.pdf', bbox_inches='tight')#, pad_inches=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The proportions of pollen not falling into one of the groups represented in the model above is deeped acceptable, i.e. at least 90% of pollen for simulated study sites is attributed to a modelled land cover type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summarise `epd_data` to land cover type level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lct_data = (\n",
    "    epd_data\n",
    "    .groupby(level=['sitecode', 'agebp', 'lct'])['species_pct'].sum()\n",
    "    .unstack().replace(np.nan, 0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lct_data.to_csv(TMP_DIR / 'site_uninterpolated_lct_ts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lct_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Interpolate data to achieve annual temporal resolution\n",
    "\n",
    "In this section we develop functions to create a new `DataFrame` based on `lct_dat` -- `interp_df` -- which will hold interpolated data derived from `lct_data` at annual temporal resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del lct_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    lct_data\n",
    "except NameError:\n",
    "    lct_data = (\n",
    "        pd.read_csv(TMP_DIR /  'site_uninterpolated_lct_ts.csv')\n",
    "        .set_index(['sitecode', 'agebp'])\n",
    "    )   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lct_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Develop a function to create an interpolated DataFrame for a single site\n",
    "\n",
    "As an example, use Algendar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lct_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algendar = lct_data.loc[pd.IndexSlice['algendar', :], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algendar.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algendar.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_lct_data(site_lct_df: pd.DataFrame):\n",
    "    \"\"\"Interpolate LCT percentage DF for site to annual time steps.\n",
    "    \n",
    "    Resulting rows are normalised to ensure total for each sample equals\n",
    "    100%.\n",
    "    \n",
    "    Input DF should have index levels ('sitecode', 'agebp').\n",
    "    \"\"\"\n",
    "    site_lct_df = site_lct_df.copy().reset_index(level='sitecode')\n",
    "    earliest, latest = site_lct_df.index.max(), site_lct_df.index.min()\n",
    "    if len(site_lct_df['sitecode'].unique()) > 1:\n",
    "        raise ValueError(\n",
    "            'It is only appropriate to interpolate values for a single site'    \n",
    "    )\n",
    "    site = site_lct_df.iloc[0]['sitecode']\n",
    "\n",
    "    return (\n",
    "        site_lct_df\n",
    "        .reindex(np.arange(latest, earliest + 1))\n",
    "        .assign(sitecode=site)\n",
    "        .set_index('sitecode', append=True).swaplevel()\n",
    "        .interpolate(method='linear')\n",
    "        .pipe(_check_all_positive)\n",
    "        .transform(_normalise_lct_row, axis=1)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _check_all_positive(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Check all values in input dataframe are positive.\n",
    "    \n",
    "    Raise value error if there are negative values, else return\n",
    "    original dataframe unchanged.\n",
    "    \"\"\"\n",
    "    if not (df >= 0).all(axis=None):\n",
    "        raise ValueError('Negative values found in DataFrame')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _normalise_lct_row(row: pd.Series, tolerance: float=0) -> pd.Series:\n",
    "    \"\"\"Ensure each sample's LCT percentages total 100%.\n",
    "    \n",
    "    Normalise if necessary.\n",
    "    \"\"\"\n",
    "    tot = row.sum()\n",
    "    if abs(tot - 100) > tolerance:\n",
    "        return (row / tot) * 100\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_interpolate_lct_data(algendar_lct_df):\n",
    "    res_df = (\n",
    "        algendar_lct_df\n",
    "        .drop(columns='not_specified')\n",
    "        .pipe(interpolate_lct_data)\n",
    "    )\n",
    "    assert res_df.iloc[0].name[1] == 2262\n",
    "    assert res_df.iloc[-1].name[1] == 8961\n",
    "    assert len(res_df.index) == 8961 - 2262 + 1\n",
    "    assert (res_df.sum(1) - 100 < 0.00001).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_interpolate_lct_data(algendar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create interpolated DataFrame for all sites\n",
    "\n",
    "Interpolate land cover proportion data at the temporal resolution provided by the European Pollen Database and produce outputs for each study site at annual resolution.\n",
    "\n",
    "The aim here is to loop through all the sites in the `all_sites` DataFrame's `sitecode` index and create an interpolated version using `interpolate_lct_data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp_lct_data = (\n",
    "    lct_data\n",
    "    .drop(columns='not_specified')\n",
    "    .groupby('sitecode')\n",
    "    .apply(interpolate_lct_data)\n",
    "    .droplevel(0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp_lct_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save resulting DataFrame to file for easy subsequent retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp_lct_data.to_csv(TMP_DIR / 'site_interpolated_lct_ts.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate time derivatives for each study site's pollen proportions\n",
    "\n",
    "Calculate first and second time derivatives (i.e. slopes) for this interpolated data.\n",
    "\n",
    "Reload interpolated data from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    interp_lct_data\n",
    "except NameError:\n",
    "    interp_lct_data = (\n",
    "        pd.read_csv(TMP_DIR / 'site_interpolated_lct_ts.csv')\n",
    "        .set_index(['sitecode', 'agebp'])\n",
    "    )   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally a gradient is given by \n",
    "\n",
    "$\\text{Grad} = \\frac{\\Delta f}{\\Delta t}$\n",
    "\n",
    "However, because in this case $\\Delta t$ is always 1 (because the resolution of the interpolated DataFrame is 1 year, the gradient is simply given by the difference between each cell and the previous one in the same column. Hence first derivatives can be calculated as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deriv_dict = dict()\n",
    "deriv_dict['pct'] = interp_lct_data\n",
    "deriv_dict['d1_pct'] = (interp_lct_data.groupby('sitecode')\n",
    "                        .apply(lambda df: df.diff()))\n",
    "deriv_dict['d2_pct'] = (deriv_dict['d1_pct'].groupby('sitecode')\n",
    "                        .apply(lambda df: df.diff()))\n",
    "deriv_lct_data = pd.concat(deriv_dict, axis=1)\n",
    "del deriv_dict\n",
    "deriv_lct_data.columns = ['_'.join(col).strip() \n",
    "                          for col in deriv_lct_data.columns.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deriv_lct_data.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, write time series files for each study site to the outputs directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for site, df in deriv_lct_data.groupby('sitecode'):\n",
    "    site_dir = OUTPUT_DIR / site\n",
    "    site_dir.mkdir(exist_ok=True)\n",
    "    df.droplevel(0).to_csv(site_dir / 'lct_pct_ts.csv')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
