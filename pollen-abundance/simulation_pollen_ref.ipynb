{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pollen reference data for simulations\n",
    "\n",
    "The objective of this notebook is to process the pandas dataframe generated in the notebook `/home/andrew/Documents/phd/data-proc/pollen-timeseries/pyogeo/notebooks/pollen-analysis/pollen-analysis.ipynb` (see [here](pollen-analysis.html) for static reference) to generate time series  for each study site with an annual temporal resolution for use in comparison to the outputs of simulation model runs. \n",
    "\n",
    "A copy of the aforementioned dataframe is stored in the file `data/0_pollen_timeseries.pkl` and will be the starting point for the following analysis:\n",
    "\n",
    "1. Interpolate land cover proportion data at the temporal resolution provided by the European Pollen Database and produce outputs for each study site at annual resolution.\n",
    "\n",
    "2. Calculate first and second time derivatives (i.e. slopes) for this interpolated data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Discard data we don't need for this processing step\n",
    "\n",
    "First load `data/0_pollen_timeseries.pkl` into a `pandas.DataFrame` and inspect available study sites:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sites = pd.read_pickle(\"data/0_pollen_timeseries.pkl\")\n",
    "\n",
    "def print_included_sites(sites_df):\n",
    "    print \"All included study sites:\"\n",
    "    for s in all_sites.index.get_level_values(\"sitecode\").unique():\n",
    "        print \"- {0}\".format(s)\n",
    "        \n",
    "print_included_sites(all_sites)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the version of `pollen_timeseries.pkl` loaded on 22/1/19, there were additional sites (bajondillo, puerto_de_los_tornos etc) which were not currently under investigation. For the remainder of this analysis I will consider only sites explicitly discussed during my upgrade:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "included_sites = ['monte_areo_mire', 'atxuri', 'charco_da_candieira', \n",
    "                  'navarres', 'algendar', 'san_rafael']\n",
    "\n",
    "all_sites = all_sites.loc[included_sites]\n",
    "\n",
    "print_included_sites(all_sites)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the index names, we notice the inclusion of the `e_` index, which identifies an individual sediment core in the EPD. Let's look at a summary how how site codes relate to sediment core numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sites_summary = all_sites.reset_index()[all_sites.index.names]\n",
    "sites_summary.columns = sites_summary.columns.droplevel(1)\n",
    "sites_summary = sites_summary.groupby(by=[\"sitecode\", \"e_\"]).count()\n",
    "sites_summary.columns = ['no_samples']\n",
    "sites_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each site has only one core associated with it, so the `e_` index can be dropped without losing any information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sites = all_sites.reset_index().drop(\"e_\", axis=1).set_index(['sitecode', 'agebp'])\n",
    "all_sites.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the data type of the `agebp` index is a float. Check if this can be safely converted to an integer value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_values = np.array(all_sites.index.get_level_values(\"agebp\").unique())\n",
    "rounded_index_values = np.rint(index_values)\n",
    "rounded_index_dif = index_values - rounded_index_values\n",
    "print \"Largest difference between raw and rounded values: \" + \\\n",
    "    str(max(rounded_index_dif.max(), abs(rounded_index_dif.min())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This demonstrates that agebp can be made an integer index without losing any information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sites.index = all_sites.index.set_levels(\n",
    "    all_sites.index.levels[1].map(\n",
    "        lambda ix: np.rint(ix).astype(\"int\")), \"agebp\")\n",
    "all_sites.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally note that for the purpose of deriveing pollen proportion time series, we don't actually need the `pcount` columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sites = all_sites.drop(\"pcount\", axis=1)\n",
    "all_sites.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Interpolate data to achieve annual temporal resolution\n",
    "\n",
    "In this section we develop functions to create a new `DataFrame` based on `all_sites` -- `interp_df` -- which will hold interpolated data derived from `all_sites` at annual temporal resolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Develop a function to create an interpolated DataFrame for a single site\n",
    "\n",
    "As an example, use Algendar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algendar = all_sites.loc['algendar']\n",
    "algendar.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine first and last `agebp` values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earliest_date = algendar.index.max()\n",
    "latest_date = algendar.index.min()\n",
    "\n",
    "print \"Earliest date: {0} yr BP\\nLatest date: {1} yr BP\".format(earliest_date, \n",
    "                                                                latest_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Derive new index based on this range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_index_vals = np.arange(latest_date, earliest_date+1)\n",
    "print new_index_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algendar_interp = pd.DataFrame(algendar.iloc[0:0], index=new_index_vals)\n",
    "algendar_interp.index.name = \"agebp\"\n",
    "algendar_interp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop through the DataFrame at EPD resolution and assign those rows for which there is data in the EPD to the correct row in the interpolated dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in algendar.iterrows():\n",
    "    algendar_interp.loc[i] = row  \n",
    "    \n",
    "algendar_interp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpolate missing data in each of the `pprop` columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in algendar_interp.columns.get_level_values(\"lct\"):\n",
    "    algendar_interp[\"pprop\", c] = algendar_interp[\"pprop\", c] \\\n",
    "                                    .interpolate(method=\"linear\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirm no entry is less than 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if algendar_interp.min().min() < 0:\n",
    "    raise ValueError(\"Negative proportions are invalid:\\n\" \\\n",
    "                     + str(algendar_interp.min()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algendar_interp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Renormalise land cover proportions to ensure that each row totals 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def renormalise_prop_row(row, tolerance=0):\n",
    "    \"\"\"Ensure LCT proportions add up to 1, normalise if not.\"\"\"\n",
    "    tot = row.sum()\n",
    "    if abs(tot-1) > tolerance:\n",
    "        return row/tot\n",
    "    return row\n",
    "\n",
    "algendar_interp = algendar_interp.apply(renormalise_prop_row, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algendar_interp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algendar_totals = algendar_interp.sum(axis=1)\n",
    "assert len(algendar_interp[algendar_totals != 1]) == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put the above logic together in a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_site_pprop(site_pprop_df):\n",
    "    \"\"\"Take EPD resolution data and interpolate to annual resolution.\n",
    "    \n",
    "    Args:\n",
    "        ssite_pprop_df (:obj:`pandas.DataFrame`): EPD time resolution data for\n",
    "            a single study site\n",
    "            \n",
    "    Returns:\n",
    "        :obj:`pandas.DataFrame`: A new `DataFrame` with the same columns as \n",
    "            the input, but interpolated so it has an annual temporal \n",
    "            resolution.\n",
    "    \"\"\"\n",
    "    # Infer earliest and latest dates in site DataFrame\n",
    "    earliest_date = site_pprop_df.index.max()\n",
    "    latest_date = site_pprop_df.index.min()\n",
    "    \n",
    "    # Derive new index based on this range\n",
    "    new_index_vals = np.arange(latest_date, earliest_date+1)\n",
    "\n",
    "    # Create new DataFrame with correct columns and index but no data\n",
    "    interp_df = pd.DataFrame(site_pprop_df.iloc[0:0], index=new_index_vals)\n",
    "    interp_df.index.name = \"agebp\"\n",
    "\n",
    "    # Load data from EPD into new DataFrame\n",
    "    for i, row in site_pprop_df.iterrows():\n",
    "        interp_df.loc[i] = row\n",
    "\n",
    "    # Interpolate missing data in each of the pprop columns\n",
    "    for c in interp_df.columns.get_level_values(\"lct\"):\n",
    "        interp_df[\"pprop\", c] = interp_df[\"pprop\", c] \\\n",
    "                                    .interpolate(method=\"linear\")\n",
    "\n",
    "    # Confirm no entry is less than 0\n",
    "    if algendar_interp.min().min() < 0:\n",
    "        raise ValueError(\"Negative proportions are invalid:\\n\" \\\n",
    "                         + str(algendar_interp.min()))\n",
    "\n",
    "    # Renormalise land cover proportions to ensure that each row totals 1.0\n",
    "    def renormalise_prop_row(row, tolerance=0):\n",
    "        \"\"\"Ensure LCT proportions add up to 1, normalise if not.\"\"\"\n",
    "        tot = row.sum()\n",
    "        if abs(tot-1) > tolerance:\n",
    "            return row/tot\n",
    "        return row\n",
    "\n",
    "    interp_df = interp_df.apply(renormalise_prop_row, axis=1)\n",
    "    \n",
    "    # Confirm land cover proportions are sufficiently close to 1.0\n",
    "    interp_df_totals = interp_df.sum(axis=1)\n",
    "    \n",
    "    if len(interp_df[interp_df_totals - 1 > 0.001]) != 0:\n",
    "        raise ValueError(\"Not all LCT proportions add up to 1:\\n\" \\\n",
    "                        + str(interp_df[interp_df_totals != 1]))\n",
    "        \n",
    "    return interp_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test on San Rafael data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "san_raf_interp = interpolate_site_pprop(all_sites.loc[\"algendar\"])\n",
    "san_raf_interp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Create interpolated DataFrame for all sites\n",
    "\n",
    "The aim here is to loop through all the sites in the `all_sites` DataFrame's `sitecode` index, create an interpolated version using `interpolate_site_pprop` and join them all together. This can be done with standard `pandas` methods for [concatenating objects](https://pandas.pydata.org/pandas-docs/stable/merging.html#concatenating-objects)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sites = all_sites.index.get_level_values(\"sitecode\").unique()\n",
    "all_sites_interp = pd.concat([interpolate_site_pprop(all_sites.loc[s]) \n",
    "                              for s in sites], keys=sites)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save resulting DataFrame to a pickle for easy subsequent retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sites_interp.to_pickle(\"data/1_all_sites_interp.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Calculate time derivatives for each study site's pollen proportions\n",
    "\n",
    "Reload interpolated data from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sites_interp = pd.read_pickle(\"data/1_all_sites_interp.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally a gradient is given by \n",
    "\n",
    "$\\text{Grad} = \\frac{\\Delta f}{\\Delta t}$\n",
    "\n",
    "However, because in this case $\\Delta t$ is always 1 (because the resolution of the interpolated DataFrame is 1 year, the gradient is simply given by the difference between each cell and the previous one in the same column. Hence first derivatives can be calculated as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_gradient_column_group(df, src_group, tgt_group):\n",
    "    for site in df.index.get_level_values(\"sitecode\").unique():\n",
    "        for lct in df.columns.get_level_values(\"lct\"):\n",
    "            df.loc[site, (tgt_group, lct)] \\\n",
    "                = df.loc[site, (src_group, lct)].diff().values\n",
    "    return df\n",
    "\n",
    "all_sites_derivs = make_gradient_column_group(all_sites_interp, \n",
    "                                              src_group=\"pprop\", \n",
    "                                              tgt_group=\"pprop_1deriv\")\n",
    "\n",
    "all_sites_derivs = make_gradient_column_group(all_sites_derivs, \n",
    "                                              src_group=\"pprop_1deriv\", \n",
    "                                              tgt_group=\"pprop_2deriv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sites_derivs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sites_derivs.to_pickle(\"data/2_all_sites_with_derivatives.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Write zipped CSV files for each study site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sites_derivs = pd.read_pickle(\"data/2_all_sites_with_derivatives.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from zipfile import ZipFile, ZIP_DEFLATED\n",
    "from io import BytesIO\n",
    "zip_dir = \"data/3_single_sites_with_derivatives\"\n",
    "if not os.path.isdir(zip_dir):\n",
    "    os.makedirs(zip_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sites = all_sites_derivs.index.get_level_values(\"sitecode\").unique()\n",
    "col_groups = all_sites_derivs.columns.get_level_values(0).unique()\n",
    "\n",
    "for site in sites:\n",
    "    with ZipFile(os.path.join(zip_dir, site + \"_pollen_timeseries.zip\"), 'w', \n",
    "                 ZIP_DEFLATED) as z:\n",
    "        for cols in col_groups:\n",
    "            string_buffer = BytesIO()\n",
    "            all_sites_derivs.loc[site, cols].to_csv(string_buffer, \n",
    "                                                    float_format='%.15f')\n",
    "            z.writestr(cols + '.csv', string_buffer.getvalue())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. [Experimental] Write HDF5 files for each study site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sites_derivs = pd.read_pickle(\"data/2_all_sites_with_derivatives.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make directory for outputs if it doesn't exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "zip_dir = \"data/3_single_sites_with_derivatives\"\n",
    "if not os.path.isdir(hdf5_dir):\n",
    "    os.makedirs(hdf5_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for site in all_sites_derivs.index.get_level_values(\"sitecode\").unique():\n",
    "    all_sites_derivs.loc[site].to_hdf(\n",
    "        os.path.join(hdf5_dir, site + \"_pollen_timeseries.h5\"), key=\"data\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pollen",
   "language": "python",
   "name": "pollen"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
